{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "#import libraries\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from keras.preprocessing import text, sequence\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import string\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "pd.set_option('display.max_columns',11)\n",
    "data = open('',encoding='utf8').read()\n",
    "labels, texts= [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):  # enumerates entries, splits each tweet into a list item\n",
    "    content = line.split()\n",
    "    labels.append(content[0]) # get labels\n",
    "    texts.append(\" \".join(content[1:])) # get text\n",
    "    \n",
    "\n",
    "def process_file(text):\n",
    "    #put text in all lower case letters \n",
    "    #all_text = text.lower()\n",
    "    all_text = text\n",
    "\n",
    "    #remove all non-alphanumeric chars\n",
    "    #all_text = re.sub(r\"[^a-zA-Z0-9]\", \" \", all_text)\n",
    "    #remove newlines/tabs, etc. so it's easier to match phrases, later\n",
    "    all_text = re.sub(r\"\\t\", \" \", all_text)\n",
    "    all_text = re.sub(r\"\\n\", \" \", all_text)\n",
    "    all_text = re.sub(\"  \", \" \", all_text)\n",
    "    all_text = re.sub(\"   \", \" \", all_text)\n",
    "    all_text = re.sub(r'(@USER)', ' ', all_text)\n",
    "    return all_text\n",
    "\n",
    "def create_text_column(df):\n",
    "  #create copy to modify\n",
    "  text_df = df.copy()\n",
    "    \n",
    "  #store processed text\n",
    "  response = []\n",
    "    \n",
    "    \n",
    "      # for each file (row) in the df, read in the file \n",
    "  for row_i in df.index:\n",
    "      filename = df.iloc[row_i]['response']\n",
    "      file_text = process_file(str(filename))\n",
    "          #append processed text to list\n",
    "      response.append(file_text)\n",
    "    \n",
    "    #add column to the copied dataframe\n",
    "  text_df['response'] = response\n",
    "\n",
    "  context_0 = []\n",
    "  for row_i in df.index:\n",
    "      filename = df.iloc[row_i]['context/0']\n",
    "      file_text = process_file(str(filename))\n",
    "          #append processed text to list\n",
    "      context_0.append(file_text)\n",
    "    \n",
    "    #add column to the copied dataframe\n",
    "  text_df['context/0']=context_0\n",
    "  context_1= []\n",
    "  for row_i in df.index:\n",
    "      filename = df.iloc[row_i]['context/1']\n",
    "      file_text = process_file(str(filename))\n",
    "          #append processed text to list\n",
    "      context_1.append(file_text)\n",
    "    \n",
    "    #add column to the copied dataframe\n",
    "  text_df['context/1']= context_1\n",
    "                                \n",
    "  concat_tweet = []\n",
    "  for row_i in df.index:\n",
    "      filename = df.iloc[row_i]['concat_tweet']\n",
    "      file_text = process_file(str(filename))\n",
    "          #append processed text to list\n",
    "      concat_tweet.append(file_text)\n",
    "    \n",
    "    #add column to the copied dataframe\n",
    "  text_df['concat_tweet']= concat_tweet                           \n",
    "                                \n",
    "    \n",
    "  return text_df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "trainDF['tokenized_text'] = trainDF.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count features as count vector\n",
    "## load the dataset\n",
    "data = open('').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")): # enumerates entries\n",
    "    content = line.split()\n",
    "    labels.append(content[0]) # get labels\n",
    "    texts.append(\" \".join(content[1:])) # get text\n",
    "\n",
    "# create a dataframe using texts and lables\n",
    "trainDF = pandas.DataFrame() \n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels\n",
    "\n",
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y =  model_selection.train_test_split(trainDF['text'],trainDF['label'], train_size=0.6,test_size=0.4)\n",
    "\n",
    "\n",
    "# label encode the target variable for ML --> numeric\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "\n",
    "# create a matrix. count_vect: every row represents a document from the corpus;\n",
    "# every column is a term from the corpus;\n",
    "# every cell represents the freq count of a particular term in particular document\n",
    " \n",
    "# create a count vectorizer object for the training data\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}') # word that appears 1 or more times\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object: doc x term\n",
    "xtrain_count =  count_vect.transform(train_x) \n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "\n",
    "#### Extracting features ########\n",
    "\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000) # considers the top 5000 most frequent features\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
    "\n",
    "## Word embeddings \n",
    "\n",
    "# https://fasttext.cc/docs/en/english-vectors.html\n",
    "# load the pre-trained word-embedding vectors \n",
    "\n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('wiki-news-300d-1M.vec')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "\n",
    "trainDF['char_count'] = trainDF['text'].apply(len)\n",
    "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
    "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
    "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIWC\n",
    "\n",
    "\n",
    "from liwc import Liwc\n",
    "import csv \n",
    "\n",
    "lwc = Liwc(\"liwc_dictionaries_shared/LIWC2007_English100131.dic\")\n",
    "liwcresults =[]\n",
    "\n",
    "for token in trainDF['tokenized_text']:\n",
    "    results = (lwc.parse(token))\n",
    "    liwcresults.append(results)\n",
    "\n",
    "liwc_keys = []\n",
    "for i in range(len(liwcresults)):\n",
    "    for k in liwcresults[i].keys():\n",
    "        if k not in liwc_keys:\n",
    "            liwc_keys.append(k)\n",
    "print(liwc_keys)\n",
    "    \n",
    "\n",
    "#look at each category in range 3,000, if the value is not in the list of keys, append new key. \n",
    "\n",
    "with open('.csv', 'w') as csv_file:  \n",
    "    dict_writer = csv.DictWriter(csv_file, liwc_keys)       \n",
    "    writer = csv.writer(csv_file)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(liwcresults)\n",
    "    \n",
    "liwcdata = pd.read_csv(\".csv\") \n",
    "liwcdata.head()\n",
    "trainDF.head()\n",
    "\n",
    "trainDF = pd.concat([liwcdata, trainDF], axis=1, sort=False)\n",
    "\n",
    "trainDF.fillna(0, inplace=True)\n",
    "#trainDF1 = trainDF1.astype(float)\n",
    "\n",
    "trainDF.to_csv('liwc.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VADER\n",
    "\n",
    "\n",
    "def vader(sentence):\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    score = vader.polarity_scores(sentence)\n",
    "    return score\n",
    "\n",
    "# Lits to generate a positive, negative, neutral, and compound score\n",
    "vs_compound = []\n",
    "vs_pos = []\n",
    "vs_neu = []\n",
    "vs_neg = []\n",
    "\n",
    "\n",
    "for row in trainDF[\"text\"]:\n",
    "    score = vader(row)\n",
    "\n",
    "    neg = float(score['neg'])\n",
    "    vs_neg.append(neg)\n",
    "    neu =float(score['neu'])\n",
    "    vs_neu.append(neu)\n",
    "    pos =float(score['pos'])\n",
    "    vs_pos.append(pos)\n",
    "    compound = float((score['compound']+1)/2) # rescaling to the [0,1] range\n",
    "    vs_compound.append(compound)\n",
    "   # vader_df = pd.DataFrame(trainDF[\"text\"])      \n",
    "\n",
    "trainDF[\"vader_pos\"] = vs_pos\n",
    "trainDF[\"vader_neg\"] = vs_neg\n",
    "trainDF[\"vader_neu\"] = vs_neu\n",
    "trainDF[\"vader_compound\"] = vs_compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAD\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import statistics\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP()\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "anew = \"../lib/EnglishShortened.csv\"\n",
    "\n",
    "\n",
    "# performs sentiment analysis on inputFile using the ANEW database, outputting results to a new CSV file in outputDir\n",
    "def analyzefile(input_file, output_dir, mode):\n",
    "    \"\"\"\n",
    "    Performs sentiment analysis on the text file given as input using the ANEW database.\n",
    "    Outputs results to a new CSV file in output_dir.\n",
    "    :param input_file: path of .txt file to analyze\n",
    "    :param output_dir: path of directory to create new output file\n",
    "    :param mode: determines how sentiment values for a sentence are computed (median or mean)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    output_file = os.path.join(output_dir, \"Output Anew Sentiment \" + os.path.basename(input_file).rstrip('.txt') + \".csv\")\n",
    "\n",
    "    # read file into string\n",
    "    with open(input_file, 'r') as myfile:\n",
    "        fulltext = myfile.read()\n",
    "    # end method if file is empty\n",
    "    if len(fulltext) < 1:\n",
    "        print('Empty file.')\n",
    "        return\n",
    "\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "\n",
    "    # otherwise, split into sentences\n",
    "    sentences = tokenize.sent_tokenize(fulltext)\n",
    "    i = 1 # to store sentence index\n",
    "    # check each word in sentence for sentiment and write to output_file\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['Sentence ID', 'Sentence', 'Sentiment', 'Sentiment Label', 'Arousal', 'Dominance',\n",
    "                      '# Words Found', 'Found Words', 'All Words']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # analyze each sentence for sentiment\n",
    "        for s in sentences:\n",
    "            # print(\"S\" + str(i) +\": \" + s)\n",
    "            all_words = []\n",
    "            found_words = []\n",
    "            total_words = 0\n",
    "            v_list = []  # holds valence scores\n",
    "            a_list = []  # holds arousal scores\n",
    "            d_list = []  # holds dominance scores\n",
    "\n",
    "            # search for each valid word's sentiment in ANEW\n",
    "            words = nlp.pos_tag(s.lower())\n",
    "            for index, p in enumerate(words):\n",
    "                # don't process stops or words w/ punctuation\n",
    "                w = p[0]\n",
    "                pos = p[1]\n",
    "                if w in stops or not w.isalpha():\n",
    "                    continue\n",
    "\n",
    "                # check for negation in 3 words before current word\n",
    "                j = index-1\n",
    "                neg = False\n",
    "                while j >= 0 and j >= index-3:\n",
    "                    if words[j][0] == 'not' or words[j][0] == 'no' or words[j][0] == 'n\\'t':\n",
    "                        neg = True\n",
    "                        break\n",
    "                    j -= 1\n",
    "\n",
    "                # lemmatize word based on pos\n",
    "                if pos[0] == 'N' or pos[0] == 'V':\n",
    "                    lemma = lmtzr.lemmatize(w, pos=pos[0].lower())\n",
    "                else:\n",
    "                    lemma = w\n",
    "\n",
    "                all_words.append(lemma)\n",
    "\n",
    "                # search for lemmatized word in ANEW\n",
    "                with open(anew) as csvfile:\n",
    "                    reader = csv.DictReader(csvfile)\n",
    "                    for row in reader:\n",
    "                        if row['Word'].casefold() == lemma.casefold():\n",
    "                            if neg:\n",
    "                                found_words.append(\"neg-\"+lemma)\n",
    "                            else:\n",
    "                                found_words.append(lemma)\n",
    "                            v = float(row['valence'])\n",
    "                            a = float(row['arousal'])\n",
    "                            d = float(row['dominance'])\n",
    "\n",
    "                            if neg:\n",
    "                                # reverse polarity for this word\n",
    "                                v = 5 - (v - 5)\n",
    "                                a = 5 - (a - 5)\n",
    "                                d = 5 - (d - 5)\n",
    "\n",
    "                            v_list.append(v)\n",
    "                            a_list.append(a)\n",
    "                            d_list.append(d)\n",
    "\n",
    "            if len(found_words) == 0:  # no words found in ANEW for this sentence\n",
    "                writer.writerow({'Sentence ID': i,\n",
    "                                 'Sentence': s,\n",
    "                                 'Sentiment': 'N/A',\n",
    "                                 'Sentiment Label': 'N/A',\n",
    "                                 'Arousal': 'N/A',\n",
    "                                 'Dominance': 'N/A',\n",
    "                                 '# Words Found': 0,\n",
    "                                 'Found Words': 'N/A',\n",
    "                                 'All Words': all_words\n",
    "                                 })\n",
    "                i += 1\n",
    "            else:  # output sentiment info for this sentence\n",
    "\n",
    "                # get values\n",
    "                if mode == 'median':\n",
    "                    sentiment = statistics.median(v_list)\n",
    "                    arousal = statistics.median(a_list)\n",
    "                    dominance = statistics.median(d_list)\n",
    "                else:\n",
    "                    sentiment = statistics.mean(v_list)\n",
    "                    arousal = statistics.mean(a_list)\n",
    "                    dominance = statistics.mean(d_list)\n",
    "\n",
    "                # set sentiment label\n",
    "                label = 'neutral'\n",
    "                if sentiment > 6:\n",
    "                    label = 'positive'\n",
    "                elif sentiment < 4:\n",
    "                    label = 'negative'\n",
    "\n",
    "                writer.writerow({'Sentence ID': i,\n",
    "                                 'Sentence': s,\n",
    "                                 'Sentiment': sentiment,\n",
    "                                 'Sentiment Label': label,\n",
    "                                 'Arousal': arousal,\n",
    "                                 'Dominance': dominance,\n",
    "                                 '# Words Found': (\"%d out of %d\" % (len(found_words), len(all_words))),\n",
    "                                 'Found Words': found_words,\n",
    "                                 'All Words': all_words\n",
    "                                 })\n",
    "                i += 1\n",
    "\n",
    "\n",
    "def main(input_file, input_dir, output_dir, mode):\n",
    "    \"\"\"\n",
    "    Runs analyzefile on the appropriate files, provided that the input paths are valid.\n",
    "    :param input_file:\n",
    "    :param input_dir:\n",
    "    :param output_dir:\n",
    "    :param mode:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if len(output_dir) < 0 or not os.path.exists(output_dir):  # empty output\n",
    "        print('No output directory specified, or path does not exist')\n",
    "        sys.exit(0)\n",
    "    elif len(input_file) == 0 and len(input_dir)  == 0:  # empty input\n",
    "        print('No input specified. Please give either a single file or a directory of files to analyze.')\n",
    "        sys.exit(1)\n",
    "    elif len(input_file) > 0:  # handle single file\n",
    "        if os.path.exists(input_file):\n",
    "            analyzefile(input_file, output_dir, mode)\n",
    "        else:\n",
    "            print('Input file \"' + input_file + '\" is invalid.')\n",
    "            sys.exit(0)\n",
    "    elif len(input_dir) > 0:  # handle directory\n",
    "        if os.path.isdir(input_dir):\n",
    "            directory = os.fsencode(input_dir)\n",
    "            for file in os.listdir(directory):\n",
    "                filename = os.path.join(input_dir, os.fsdecode(file))\n",
    "                if filename.endswith(\".txt\"):\n",
    "                    start_time = time.time()\n",
    "                    print(\"Starting sentiment analysis of \" + filename + \"...\")\n",
    "                    analyzefile(filename, output_dir, mode)\n",
    "                    print(\"Finished analyzing \" + filename + \" in \" + str((time.time() - start_time)) + \" seconds\")\n",
    "        else:\n",
    "            print('Input directory \"' + input_dir + '\" is invalid.')\n",
    "            sys.exit(0)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # get arguments from command line\n",
    "    parser = argparse.ArgumentParser(description='Sentiment analysis with ANEW.')\n",
    "    parser.add_argument('--file', type=str, dest='input_file', default='',\n",
    "                        help='a string to hold the path of one file to process')\n",
    "    parser.add_argument('--dir', type=str, dest='input_dir', default='',\n",
    "                        help='a string to hold the path of a directory of files to process')\n",
    "    parser.add_argument('--out', type=str, dest='output_dir', default='',\n",
    "                        help='a string to hold the path of the output directory')\n",
    "    parser.add_argument('--mode', type=str, dest='mode', default='mean',\n",
    "                        help='mode with which to calculate sentiment in the sentence: mean or median')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # run main\n",
    "    sys.exit(main(args.input_file, args.input_dir, args.output_dir, args.mode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emotional embeddings\n",
    "\n",
    "\n",
    "## Function to get Emotion Vectors\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
    "\n",
    "def get_emotion(text):\n",
    "  input_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\n",
    "\n",
    "  output = model.generate(input_ids=input_ids,\n",
    "               max_length=2)\n",
    "\n",
    "  dec = [tokenizer.decode(ids) for ids in output]\n",
    "  label = dec[0]\n",
    "  return input_ids\n",
    "\n",
    "\n",
    "\n",
    "#Function to get Emotion labels\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
    "\n",
    "def get_emotion_label(text):\n",
    "  input_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\n",
    "\n",
    "  output = model.generate(input_ids=input_ids,\n",
    "               max_length=2)\n",
    "\n",
    "  dec = [tokenizer.decode(ids) for ids in output]\n",
    "  label = dec[0]\n",
    "  return label\n",
    "\n",
    "\n",
    "\n",
    "##Create feature vector \n",
    "new_df[\"Response_emotion\"] = response_sent\n",
    "\n",
    "\n",
    "new_df.head()\n",
    "\n",
    "\n",
    "context_sent = []\n",
    "\n",
    "\n",
    "for row in new_df[\"context/0\"]:\n",
    "    if row == 0:\n",
    "        \n",
    "        sentiment = 'NaN'\n",
    "  \n",
    "        context_sent.append(sentiment)\n",
    "    else:\n",
    "        \n",
    "        sentiment = get_emotion_label(row)\n",
    "        context_sent.append(sentiment)\n",
    "        \n",
    "\n",
    "\n",
    "##Create feature vector \n",
    "new_df[\"context0_sentiment\"] = context_sent\n",
    "new_df.to_csv('final_test_emotions.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarity\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_cosine_similarity(feature_vec_1,feature_vec_2):\n",
    "    return cosine_similarity(feature_vec_1.reshape(1, -1), feature_vec_2.reshape(1, -1))[0][0]\n",
    "\n",
    "def get_vectors(text1, text2):\n",
    "    # Turns text into emotional tensors\n",
    "    tensor1 = get_emotion(text1)\n",
    "    tensor2 = get_emotion(text2)\n",
    "    \n",
    "    # Turns emotional tensors into numpy array\n",
    "    Vector_1 = numpy.array(tensor1)\n",
    "    Vector_2 = numpy.array(tensor2)\n",
    "    \n",
    "    #pads lower array with zeros to match higher array\n",
    "    if Vector_1.size >= Vector_2.size:\n",
    "        vA = Vector_1\n",
    "        vB = Vector_2\n",
    "        result = np.zeros(vA.shape)\n",
    "        result[:vB.shape[0],:vB.shape[1]] = vB\n",
    "        vC = vA\n",
    "    elif Vector_1.size <= Vector_2.size:\n",
    "        vB = Vector_2\n",
    "        vA = Vector_1\n",
    "        result = np.zeros(vB.shape)\n",
    "        result[:vA.shape[0],:vA.shape[1]]= vA\n",
    "        vC = vB\n",
    "    return vC, result\n",
    "\n",
    "a,b = get_vectors(text2,text1)\n",
    "cosine_similarities = get_cosine_similarity(a,b)\n",
    "print(cosine_similarities)\n",
    "\n",
    "\n",
    "Emotional_sim = []\n",
    "for row_a in new_df['response']:\n",
    "    if row_a == 0:\n",
    "        row_a = 'NaN'\n",
    "    else:\n",
    "        row_a = row_a\n",
    "            \n",
    "    for row_b in new_df['context/0']:\n",
    "        if row_b == 0:\n",
    "            row_b = 'NaN'\n",
    "        else:\n",
    "            row_b = row_b\n",
    "        \n",
    "        \n",
    "        text1 = row_a\n",
    "        text2 = row_b\n",
    "        \n",
    "    \n",
    "        a,b = get_vectors(text1,text2)\n",
    "        cosine_similarities = get_cosine_similarity(a,b)\n",
    "        Emotional_sim.append(cosine_similarities)\n",
    "    \n",
    "new_df[\"Emotional_sim_response/context/0\"] = Emotional_sim\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_column(df):\n",
    "\n",
    "  text_df = df.copy()\n",
    "    \n",
    "  #store processed text\n",
    "  text = []\n",
    "    \n",
    "      # for each file (row) in the df, read in the file \n",
    "  for row_i in df.index:\n",
    "      filename = df.iloc[row_i]['response']\n",
    "      file_text = process_file(str(filename))\n",
    "          #append processed text to list\n",
    "      text.append(file_text)\n",
    "    \n",
    "    #add column to the copied dataframe\n",
    "  text_df['text'] = text\n",
    "    \n",
    "  return text_df\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer  \n",
    "  \n",
    "  #Sentiment Analyzer VADER\n",
    "def nltk_sentiment(sentence):\n",
    "  nltk_sentiment = SentimentIntensityAnalyzer()\n",
    "  score = nltk_sentiment.polarity_scores(str(sentence))\n",
    "  return score\n",
    "\n",
    "  pos_family = {\n",
    "      'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "      'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "      'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "      'adj' :  ['JJ','JJR','JJS'],\n",
    "      'adv' : ['RB','RBR','RBS','WRB']\n",
    "  }\n",
    "\n",
    "  from textblob import TextBlob\n",
    "  # function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    pos_family = {\n",
    "      'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "      'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "      'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "      'adj' :  ['JJ','JJR','JJS'],\n",
    "      'adv' : ['RB','RBR','RBS','WRB']\n",
    "  }\n",
    "  \n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    " \n",
    "\n",
    "def features(df1, df2):\n",
    "  # creating lists to keep pos, neu, neg, and compound scores --- later to be used to create a dataframe\n",
    "\n",
    "  complete_df = df1\n",
    "  features_df = df2\n",
    "  vs_compound = []\n",
    "  vs_pos = []\n",
    "  vs_neu = []\n",
    "  vs_neg = []\n",
    "\n",
    "\n",
    "  for row in complete_df[\"response\"]:\n",
    "      score = nltk_sentiment(str(row))\n",
    "  \n",
    "      neg = float(score['neg'])\n",
    "      vs_neg.append(neg)\n",
    "      neu =float(score['neu'])\n",
    "      vs_neu.append(neu)\n",
    "      pos =float(score['pos'])\n",
    "      vs_pos.append(pos)\n",
    "      compound = float((score['compound']+1)/2) # rescaling to the [0,1] range\n",
    "      vs_compound.append(compound)\n",
    "        \n",
    "\n",
    "##Create feature vector \n",
    "  features_df[\"vader_pos\"] = vs_pos\n",
    "  features_df[\"vader_neg\"] = vs_neg\n",
    "  features_df[\"vader_neu\"] = vs_neu\n",
    "  features_df[\"vader_compound\"] = vs_compound\n",
    "\n",
    "  features_df['noun_count'] = complete_df['response'].apply(lambda x: check_pos_tag(x, 'noun')).astype(float)\n",
    "  features_df['verb_count'] = complete_df['response'].apply(lambda x: check_pos_tag(x, 'verb')).astype(float)\n",
    "  features_df['adj_count'] = complete_df['response'].apply(lambda x: check_pos_tag(x, 'adj')).astype(float)\n",
    "  features_df['adv_count'] = complete_df['response'].apply(lambda x: check_pos_tag(x, 'adv')).astype(float)\n",
    "  features_df['pron_count'] = complete_df['response'].apply(lambda x: check_pos_tag(x, 'pron')).astype(float)\n",
    "\n",
    "  features_df['char_count'] = complete_df['response'].apply(len).astype(float)\n",
    "  features_df['word_count'] = complete_df['response'].apply(lambda x: len(x.split())).astype(float)\n",
    "  features_df['word_density'] = features_df['char_count']/ (features_df['word_count']+1).astype(float)\n",
    "  features_df['punctuation_count'] = complete_df['response'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))).astype(float)\n",
    "  features_df['upper_case_word_count'] = complete_df['response'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()])).astype(float)\n",
    "\n",
    "  return features_df\n",
    "\n",
    "def normalized_features(df1,df2):\n",
    "  norm_features_df = df1\n",
    "  features_df = df2\n",
    "\n",
    "\n",
    "  norm_features_df['noun_count_percent'] = features_df['noun_count']/ (features_df['word_count']+1).astype(float)\n",
    "  norm_features_df['noun_count_percent'] = features_df['verb_count']/ (features_df['word_count']+1).astype(float)\n",
    "  norm_features_df['adj_count_percent'] = features_df['adj_count']/ (features_df['word_count']+1).astype(float)\n",
    "  norm_features_df['adv_count_percent'] = features_df['adv_count']/ (features_df['word_count']+1).astype(float)\n",
    "  norm_features_df['pro_count_percent'] = features_df['pron_count']/ (features_df['word_count']+1).astype(float)\n",
    "\n",
    "  norm_features_df['char_count'] = features_df['char_count']/ (features_df['word_count']+1).astype(float)\n",
    "  norm_features_df['word_density'] = features_df['word_density']/ (features_df['word_count']+1).astype(float)\n",
    "  norm_features_df['punctuation_count'] = features_df['punctuation_count']/ (features_df['word_count']+1).astype(float)\n",
    "  norm_features_df['upper_case_word_count'] = features_df['upper_case_word_count']/ (features_df['word_count']+1).astype(float)\n",
    "\n",
    "\n",
    "  norm_features_df[\"vader_pos\"] = features_df[\"vader_pos\"]\n",
    "  norm_features_df[\"vader_neg\"] = features_df[\"vader_neg\"]\n",
    "  norm_features_df[\"vader_neu\"] = features_df[\"vader_neu\"]\n",
    "  norm_features_df[\"vader_compound\"] = features_df[\"vader_compound\"]\n",
    "\n",
    "   \n",
    "\n",
    "  return norm_features_df\n",
    "\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def count_features(df):\n",
    "\n",
    "\n",
    "  count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}',max_features= 3000) # word that appears 1 or more times\n",
    "  count_vect.fit(df['response'])\n",
    "\n",
    "  # to show resulting vocabulary; the numbers are not counts, they are the position in the sparse vector.\n",
    "  count_vect.vocabulary_\n",
    "\n",
    "  # transform the training and validation data using count vectorizer object: doc x term\n",
    "  x_count =  count_vect.transform(df['response']) \n",
    "  # Convert to matrix\n",
    "  x_count = csr_matrix(x_count)\n",
    "  x_count  = x_count.todense()\n",
    "\n",
    "  return x_count\n",
    "\n",
    "def tfidf_features(df):\n",
    "\n",
    "\n",
    "### word level tf-idf\n",
    "\n",
    "\n",
    "  tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=3000) # considers the top 5000 most frequent features\n",
    "  tfidf_vect.fit(df['response'])\n",
    "\n",
    "  tfidf =  tfidf_vect.transform(df['response'])\n",
    "  tfidf = csr_matrix(tfidf)\n",
    "\n",
    "  tfidf  = tfidf.todense()\n",
    "\n",
    "  return tfidf\n",
    "\n",
    "def tfidf_ngram_features(df):\n",
    "\n",
    "  ### ngram level tf-idf\n",
    "\n",
    "  tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,2), max_features=3000)\n",
    "  tfidf_vect_ngram.fit(df['response'])\n",
    "\n",
    "  tfidf_ngram =  tfidf_vect_ngram.transform(df['response'])\n",
    "  tfidf_ngram = csr_matrix(tfidf_ngram)\n",
    "  tfidf_ngram  = tfidf_ngram.todense()\n",
    "\n",
    "  return tfidf_ngram\n",
    "\n",
    "def tfidf_ngram_chars(df):\n",
    "\n",
    "  ### characters level tf-idf\n",
    "\n",
    "  tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(1,2), max_features=1500)\n",
    "  tfidf_vect_ngram_chars.fit(df['response'])\n",
    "\n",
    "  tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(df['response']) \n",
    "\n",
    "  tfidf_ngram_chars = csr_matrix(tfidf_ngram_chars)\n",
    "  tfidf_ngram_chars  = tfidf_ngram_chars.todense()\n",
    "\n",
    "  return tfidf_ngram_chars\n",
    "\n",
    "def ling_feature_vec(norm_features_df):\n",
    "  from numpy import array\n",
    "  from scipy.sparse import csr_matrix\n",
    "  x = norm_features_df.copy()\n",
    "\n",
    "  Features = x.drop(['response', 'type', 'subject','title','date', 'id' ],axis=1)\n",
    "\n",
    "  Features = Features.to_numpy()\n",
    "\n",
    "  Features = csr_matrix(Features)\n",
    "\n",
    "  # reconstruct dense matrix\n",
    "  Features  = Features .todense()\n",
    "  return Features\n",
    "\n",
    "def all_count_features(tfidf,tfidf_ngram,tfidf_ngram_chars, x_count):\n",
    "  all_count_features = np.column_stack([tfidf,tfidf_ngram,tfidf_ngram_chars, x_count])\n",
    "\n",
    "  return all_count_features\n",
    "\n",
    "def all_features(x_count,tfidf,tfidf_ngram,tfidf_ngram_chars, Features):\n",
    "  all_features = np.column_stack([x_count,tfidf,tfidf_ngram,tfidf_ngram_chars, Features])\n",
    "  return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAPLEY score\n",
    "\n",
    "shap.initjs()\n",
    "xgb_model = XGBRegressor(n_estimators=1000, max_depth=10, learning_rate=0.001, random_state=0)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_predict = xgb_model.predict(X_test)\n",
    "mean_squared_error(y_test, y_predict)**(0.5)\n",
    "\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "i = 5000\n",
    "shap.force_plot(explainer.expected_value, shap_values[i], features=X_train.loc[5000], feature_names=X_train.columns)\n",
    "\n",
    "shap.summary_plot(shap_values, features=X_train, feature_names=X_train.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier #random forest #different experiments\n",
    "\n",
    "accuracy, precision, recall,fl  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), xtrain_count, y_train, xvalid_count)\n",
    "print (\"random Forest: \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
